import os, logging

from threading import Thread, Lock
from uuid import uuid4
from pathlib import Path

from tasks.TaskExecutorFactory import task_executor_factory as factory
from owe_python_sdk.TaskResult import TaskResult
from owe_python_sdk.TaskExecutor import TaskExecutor
from owe_python_sdk.constants import STDERR, STDOUT
from owe_python_sdk.middleware.ArchiveMiddleware import ArchiveMiddleware
from owe_python_sdk.events import (
    Event,
    EventPublisher,
    EventExchange,
    ExchangeConfig
)
from owe_python_sdk.events.types import (
    PIPELINE_ACTIVE, PIPELINE_COMPLETED, PIPELINE_FAILED, PIPELINE_TERMINATED,
    PIPELINE_STAGING, TASK_STAGING, TASK_ACTIVE, TASK_COMPLETED, TASK_FAILED,
    TASK_SKIPPED
)
from templating import TemplateMapper
from errors.tasks import (
    InvalidTaskTypeError,
    MissingInitialTasksError,
    InvalidDependenciesError,
    CycleDetectedError,
    ConditionalExpressionEvalError,
    TaskInputStagingError
)
from middleware.archivers import S3Archiver, IRODSArchiver
from conf.constants import BASE_WORK_DIR
from workers import Worker
from state import Hook, method_hook

from workflows import params_validator
from ioc import IOCContainerFactory
from utils import lbuffer_str as lbuf, CompositeLogger


server_logger = logging.getLogger("server")

def interruptable(rollback=None): # Decorator factory
    def interruptable_decorator(fn): # Decorator
        def wrapper(self, *args, **kwargs): # Wrapper
            rollback_fn = getattr(self, (rollback or ""), None)
            try:
                # TODO figure out why setting reactive state in this decorator causes
                # a threading.Lock issue!
                res = fn(self, *args, **kwargs)
                if self.state.terminating or self.state.terminated:
                    rollback_fn and rollback_fn()

                return res
            except Exception as e:
                server_logger.debug(f"Workflow Termination Signal Detected: Terminating:{self.state.terminating}/Terminated:{self.state.terminated}")
                if self.state.terminating or self.state.terminated:
                    # Run the rollback function by the name provided in the
                    # interruptable decorator factory args
                    rollback_fn and rollback_fn()
                    return
                raise e
            
        return wrapper

    return interruptable_decorator

class WorkflowExecutor(Worker, EventPublisher):
    """The Workflow Executor is responsible for processing and executing tasks for
    a single workflow. The entrypoint of Workflow Executor is a the 'start' method.
    
    When initialized, the WorkflowExecutor creates an EventExchange to which
    EventPublishers can publish Events. EventHandlers can then be registered to the 
    EventExchange and handle the Events generated by the EventPublishers. The 
    WorkflowExecutor itself--as well as every TaskExecutor it spawns--are capable 
    of publishing Events to this exchange. Each WorkflowExecutor initialized by
    the Server is persitent, meaning that it is used throughout the lifetime
    of the Workflow Executor Server. After each run of a workflow, the
    WorkflowExecutor and its EventExchange are reset.
    """

    def __init__(self, _id=None, plugins=[]):
        # Initialze the Worker class
        Worker.__init__(self, _id)

        # Set the plugins
        self._plugins = plugins

        # Initializes the primary(and only)event exchange, enabling publishers
        # (the WorkflowExecutor and other Event producers) to publish Events to it,
        # triggering subscribers to handle those events. 
        EventPublisher.__init__(
            self,
            EventExchange(
                config=ExchangeConfig(
                    reset_on=[PIPELINE_COMPLETED, PIPELINE_FAILED, PIPELINE_TERMINATED]
                )
            )
        )

        container_factory = IOCContainerFactory()
        self.container = container_factory.build()

        self.state = self.container.load("ReactiveState")
        self.state.set_initial_state({
            "threads": [],
            "terminated": False,
            "terminating": False,
            "failed": [],
            "failures_permitted": [],
            "succeeded": [],
            "finished": [],
            "skipped": [],
            "queue": [],
            "tasks": [],
            "executors": {},
            "dependency_graph": {},
            "ready_tasks": [],
            "ctx": None,
        })
        self.state.register_hooks(
            [
                Hook(
                    self._on_change_state,
                    attrs=[] # No attrs means all state gets/sets triggers this hook
                ),
                # NOTE Not necessary to have this as a hook, but a good it's a good
                # demonstration of how ReactiveState works. Move logic from 
                # _on_change_ready_task to all spots where self.state.ready_tasks
                # is accessed or updated and remove this Hook
                Hook(
                    self._on_change_ready_task,
                    attrs=["ready_tasks"]
                )
            ]
        )

        self._set_initial_state()
        
    # Logging formatters. Makes logs more useful and pretty
    def p_log(self, status): return f"{lbuf('[PIPELINE]')} {self.state.ctx.idempotency_key} {status} {self.state.ctx.pipeline.id}"
    def t_log(self, task, status): return f"{lbuf('[TASK]')} {self.state.ctx.idempotency_key} {status} {self.state.ctx.pipeline.id}.{task.id}"

    @interruptable()
    def start(self, ctx, threads):
        """This method is the entrypoint for a workflow exection. It's invoked
        by the main Server instance when a workflow submission is 
        recieved"""
        self.state.threads = threads

        try:
            # Prepare the workflow executor, temporary results storage,
            # middleware(notification and archivers), queue the tasks
            self._staging(ctx)

            # Validate the submission args against the pipeline's parameters
            (validated, err) = params_validator(
                self.state.ctx.pipeline.params,
                self.state.ctx.args
            )

            if not validated:
                self._handle_pipeline_terminal_state(event=PIPELINE_FAILED, message=err)
                return

            # Get the first tasks
            unstarted_threads = self._fetch_ready_tasks()

            # Log the pipeline status change
            self.state.ctx.logger.info(self.p_log("ACTIVE"))

            # Publish the active event
            self.publish(Event(PIPELINE_ACTIVE, self.state.ctx))
            
            # NOTE Triggers the hook _on_change_ready_task
            self.state.ready_tasks += unstarted_threads
        except Exception as e:
            server_logger.exception(e.__cause__)
            # Trigger the terminal state callback.
            self._handle_pipeline_terminal_state(event=PIPELINE_FAILED, message=str(e))

    @interruptable()
    def _staging(self, ctx):
        # Resets the workflow executor to its initial state
        self._set_initial_state()
        
        # Sets the execution context to the ReactiveState of the WorkflowExecutor.
        # All subsequent references to the context should be made via 'self.state.ctx'
        self._set_context(ctx)
        
        # Prepare the file system for this pipeline and handle pipeline templating
        self._prepare_pipeline()

        # Publish the PIPELINE_STAGING event
        # NOTE Events can only be published AFTER the '_prepare_pipeline' method is called
        # because the directory structure in which the logs do not exists until it is called.
        self.publish(Event(PIPELINE_STAGING, self.state.ctx))

        # Setup the server and the pipeline run loggers
        self._setup_loggers()

        # Prepare task objects and create the directory structure for task output and execution
        self._prepare_tasks()

        self.state.ctx.logger.info(f'{self.p_log("STAGING")} {self.state.ctx.pipeline_run.uuid}')

        # Notification handlers are used to relay/persist updates about a pipeline run
        # and its tasks to some external resource
        self._initialize_notification_handlers()

        # Prepares archives to which the results of workflows will be persisted
        self._initialize_archivers()

        # Validate the tasks. Kill the pipeline if it contains cycles
        # or invalid dependencies
        try:
            self._set_tasks(self.state.ctx.pipeline.tasks)
        except InvalidDependenciesError as e:
            server_logger.exception(e.__cause__)
            self._handle_pipeline_terminal_state(PIPELINE_FAILED, message=str(e))
        except Exception as e:
            server_logger.exception(e.__cause__)
            self._handle_pipeline_terminal_state(PIPELINE_FAILED, message=str(e))
        
    @interruptable()
    def _prepare_tasks(self):
        """This function adds information about the pipeline context to the task
        objects, prepares the file system for each task execution, handles task templating,
        and generates and registers the task executors that will be called to perform the
        work detailed in the task definition."""
        self.state.ctx.output = {}
        for task in self.state.ctx.pipeline.tasks:
            self.state.ctx.logger.info(self.t_log(task, "STAGING"))

            # Publish the task active event
            self.publish(Event(TASK_STAGING, self.state.ctx, task=task))

            # Set the can fail flag for each task to False
            task.can_fail = False

            # Create an execution_uuid for each task in the pipeline
            task.execution_uuid = str(uuid4())

            # Paths to the workdir for the task inside the workflow engine container
            task.work_dir = f"{self.state.ctx.pipeline.work_dir}{task.id}/"
            task.exec_dir = f"{task.work_dir}src/"
            task.input_dir = f"{task.work_dir}input/"
            task.output_dir = f"{task.work_dir}output/"
            task.stdout = f"{task.output_dir}{STDOUT}"
            task.stderr = f"{task.output_dir}{STDERR}"

            # Paths to the workdir for the task inside the job container
            task.container_work_dir = "/mnt/open-workflow-engine/pipeline/task"
            task.container_exec_dir = f"{task.container_work_dir}/src/"
            task.container_input_dir = f"{task.container_work_dir}/input/"
            task.container_output_dir = f"{task.container_work_dir}/output/"

            # Paths to the workdir inside the nfs-server container
            task.nfs_work_dir = f"{self.state.ctx.pipeline.nfs_work_dir}{task.id}/"
            task.nfs_exec_dir = f"{task.nfs_work_dir}src/"
            task.nfs_input_dir = f"{task.nfs_work_dir}input/"
            task.nfs_output_dir = f"{task.nfs_work_dir}output/"

            # Create the task's directories
            self._prepare_task_fs(task)

            # Fetch task templates
            if task.uses != None:
                template_mapper = TemplateMapper(cache_dir=self.state.ctx.pipeline.git_cache_dir)
                try:
                    task = template_mapper.map(task, task.uses)
                except Exception as e:
                    server_logger.exception(e.__cause__)
                    # Trigger the terminal state callback.
                    self._handle_pipeline_terminal_state(event=PIPELINE_FAILED, message=str(e))

            # Add a key to the output for the task
            self.state.ctx.output[task.id] = None

            # Resolve and register the task executor
            executor = factory.build(task, self.state.ctx, self.exchange, plugins=self._plugins)

            # Register the task executor
            self._register_executor(self.state.ctx.pipeline_run.uuid, task, executor)

    @interruptable()
    def _prepare_task_fs(self, task):
        # Create the base directory for all files and output created during this task execution
        os.makedirs(task.work_dir, exist_ok=True)

        # Create the exec dir for files created in support of the task execution
        os.makedirs(task.exec_dir, exist_ok=True)

        # Create the output dir in which the output of the task execution will be stored
        os.makedirs(task.output_dir, exist_ok=True)

        # Create the input dir in which the inputs to tasks will be staged
        os.makedirs(task.input_dir, exist_ok=True)

        # Create the stdout and stderr files
        Path(task.stdout).touch()
        Path(task.stderr).touch()

    @interruptable()
    def _start_task(self, task):
        # Check if any of the previous tasks were skipped. If yes, and the current
        # task's dependency specifies a can_skip == False for any of the skipped tasks,
        # this task will also be skipped
        skip = False
        for dep in task.depends_on:
            if dep.id in self.state.skipped and dep.can_skip == False:
                skip = True
                break
        
        # Default TaskResult is a task skipped. Will be overwritten if task not skipped
        task_result = TaskResult(-1)

        # Determine if the task should be skipped
        expression_error = False
        if not skip:
            try:
                # Evaluate the task's conditions if the previous task was not skipped
                evaluator = self.container.load("ConditionalExpressionEvaluator")
                skip = not evaluator.evaluate_all(task.conditions)
            except ConditionalExpressionEvalError as e:
                server_logger.exception(e.__cause__)
                expression_error = True
                task_result = TaskResult(1, errors=[str(e)])

        # Stage task inputs
        if not skip and not expression_error:
            task_input_file_staging_service = self.container.load(
                "TaskInputFileStagingService"
            )
            try:
                task_input_file_staging_service.stage(task)
            except TaskInputStagingError as e:
                # Get the next queued tasks if any
                unstarted_threads = self._handle_task_terminal_state(
                    task,
                    TaskResult(1, errors=[str(e)])
                )
                self.state.ctx.logger.info(self.t_log(task, "FAILED"))
                server_logger.exception(e.__cause__)
                self.publish(Event(TASK_FAILED, self.state.ctx, task=task))

                # NOTE Triggers hook _on_change_ready_task
                self.state.ready_tasks += unstarted_threads
                return

            # Log the task status
            self.state.ctx.logger.info(self.t_log(task, "ACTIVE"))

            # Publish the task active event
            self.publish(Event(TASK_ACTIVE, self.state.ctx, task=task))

            try:
                # Fetch the executor
                executor: TaskExecutor = self._get_executor(
                    self.state.ctx.pipeline_run.uuid,
                    task
                )

                # Execute the task
                task_result = executor.execute()

                # Set the output of the task
                self.state.ctx.output = {
                    **self.state.ctx.output,
                    **task_result.output
                }
            except InvalidTaskTypeError as e:
                self.state.ctx.logger.error(str(e))
                server_logger.exception(e.__cause__)
                task_result = TaskResult(1, errors=[str(e)])
            except Exception as e:
                self.state.ctx.logger.error(str(e))
                server_logger.exception(e.__cause__)
                task_result = TaskResult(1, errors=[str(e)])

        # Get the next queued tasks if any
        unstarted_threads = self._handle_task_terminal_state(task, task_result)

        # NOTE Triggers hook _on_change_ready_task
        self.state.ready_tasks += unstarted_threads

    @interruptable()
    def _handle_task_terminal_state(self, task, task_result):
        # Determine the correct callback to use.
        callback = self._handle_task_completed

        if task_result.skipped:
            callback = self._handle_task_skipped

        if not task_result.success and not task_result.skipped:
            callback = self._handle_task_failed

        # Call the callback. Marks task as completed or failed.
        # Also publishes a TASK_COMPLETED or TASK_FAILED based on the result
        callback(task, task_result)

        # TODO Check to see if the task has any more "retries" available.
        # If it does, requeue

        # Deregister the task executor. This cleans up the resources that were created
        # during the initialization and execution of the task executor
        self._deregister_executor(self.state.ctx.pipeline_run.uuid, task)
        
        # Run the handle_pipeline_terminal_state callback if all tasks are complete.
        if len(self.state.tasks) == len(self.state.finished):
            self._handle_pipeline_terminal_state(event=PIPELINE_COMPLETED)
            return []
        
        if task_result.status > 0 and task.can_fail == False:
            self._handle_pipeline_terminal_state(event=PIPELINE_FAILED)
            return []

        # Execute all possible queued tasks
        return self._fetch_ready_tasks()

    @interruptable()
    def _handle_pipeline_terminal_state(self, event=None, message=""):
        # No event was provided. Determine if complete or failed from number
        # of failed tasks
        if event == None:
            event = PIPELINE_FAILED if len(self.state.failed) > 0 else PIPELINE_COMPLETED

        msg = "COMPLETED"

        if event == PIPELINE_FAILED: msg = "FAILED" + f" {message}"
        elif event == PIPELINE_TERMINATED: msg = "TERMINATED" + f" {message}"

        # TODO what happens in the scenario in which all final tasks fail. Should
        # that be considered a success?? Probably not. Also, consider adding a property
        # to the pipeline.execution_profile object that allows users to control
        # how success is interpreted. 
        # Examples:
        # - pipeline.execution_profile.success_condidition = TASKS_WITH_NO_PARENT_TASKS_MUST_SUCCEED
        # - pipeline.execution_profile.success_condidition = ALL_TASKS_MUST_COMPLETE
        # - pipeline.execution_profile.success_condidition = ALL_TASKS_MUST_FAIL

        self.state.ctx.logger.info(self.p_log(msg))

        # Publish the event. Triggers the archivers if there are any on ...COMPLETE
        self.publish(Event(event, self.state.ctx))
        
        self._cleanup_run()

        self._set_initial_state() 

    @interruptable()
    def _handle_task_completed(self, task, task_result):
        # Log the completion
        self.state.ctx.logger.info(self.t_log(task, "COMPLETED"))

        # Notify the subscribers that the task was completed
        self.publish(Event(TASK_COMPLETED, self.state.ctx, task=task, result=task_result))


        # Add the task to the finished list
        self.state.finished.append(task.id)
        self.state.succeeded.append(task.id)

    @interruptable()
    def _handle_task_skipped(self, task, _):
        # Log the task active
        self.state.ctx.logger.info(self.t_log(task, "SKIPPED"))

        # Publish the task active event
        self.publish(Event(TASK_SKIPPED, self.state.ctx, task=task))

        # Add the task to the finished and skipped list
        self.state.finished.append(task.id)
        self.state.skipped.append(task.id)

    @interruptable()
    def _handle_task_failed(self, task, task_result):
        # Log the failure
        self.state.ctx.logger.info(self.t_log(task, f"FAILED: {task_result.errors}"))

        # Notify the subscribers that the task was completed
        self.publish(Event(TASK_FAILED, self.state.ctx, task=task, result=task_result))

        # Add the task to the finished list
        self.state.finished.append(task.id)
        self.state.failed.append(task.id)

    @interruptable()
    def _get_initial_tasks(self, tasks):
        initial_tasks = [task for task in tasks if len(task.depends_on) == 0]

        if len(initial_tasks) == 0:
            raise MissingInitialTasksError(
                "Expected: 1 or more tasks with no dependencies - Found: 0"
            )

        return initial_tasks
    
    @interruptable()
    def _set_tasks(self, tasks):
        # Create a list of the ids of the tasks
        task_ids = [task.id for task in tasks]

        # Determine if there are any invalid dependencies (dependencies not
        # included in the tasks list)
        invalid_deps = 0
        invalid_deps_message = ""
        for task in tasks:
            for dep in task.depends_on:
                if dep.id == task.id:
                    invalid_deps += 1
                    invalid_deps_message = (
                        invalid_deps_message
                        + f"#{invalid_deps} A task cannot be dependent on itself: {task.id} | "
                    )
                if dep.id not in task_ids:
                    invalid_deps += 1
                    invalid_deps_message = (
                        invalid_deps_message
                        + f"#{invalid_deps} Task '{task.id}' depends on non-existent task '{dep.id}'"
                    )

        if invalid_deps > 0:
            raise InvalidDependenciesError(invalid_deps_message)

        self.state.tasks = tasks

        # Build the dependency graph where the key is a task id and the value is
        # an array of all the dependent tasks
        self.state.dependency_graph = {task.id: [] for task in self.state.tasks}
        for task in self.state.tasks:
            for parent_task in task.depends_on:
                self.state.dependency_graph[parent_task.id].append(task.id)

        # Determine if a task can fail and set the tasks' can_fail flags.
        # A parent task is permitted to fail iff all of the following criteria are met:
        # - It has children
        # - All can_fail flags for a given parent task's children's task_dependency object == True 
        try:
            for parent_task_id in self.state.dependency_graph:
                parent_task = self._get_task_by_id(parent_task_id)
                child_tasks = [
                    self._get_task_by_id(child_task_id)
                    for child_task_id
                    in self.state.dependency_graph[parent_task_id]
                ]
                parent_can_fail_flags = []
                for child_task in child_tasks:
                    dep = next(filter(lambda dep: dep.id == parent_task_id, child_task.depends_on))
                    parent_can_fail_flags.append(dep.can_fail)

                # If the length of can_fail_flags == 0, then this task has no child tasks
                parent_task.can_fail = (
                    False if len(parent_can_fail_flags) == 0
                    else all(parent_can_fail_flags)
                )
        except Exception as e:
            server_logger.exception(e.__cause__)
            raise Exception(f"Error resolving can_fail flag for parent task '{parent_task_id}': {e}")

        # Detect loops in the graph
        try:
            initial_tasks = self._get_initial_tasks(self.state.tasks)
            graph_validator = self.container.load("GraphValidator")
            if graph_validator.has_cycle(self.state.dependency_graph, initial_tasks):
                raise CycleDetectedError("Cyclic dependencies detected")
        except (
            InvalidDependenciesError, CycleDetectedError, MissingInitialTasksError
        ) as e:
            server_logger.exception(e.__cause__)
            raise e

        # Add all tasks to the queue
        self.state.queue = [ task for task in self.state.tasks ]

    @interruptable()
    def _prepare_pipeline(self):
        # Create all of the directories needed for the pipeline to run and persist results and cache
        self._prepare_pipeline_fs()
            
        # TODO Perform template mapping at the pipeline level here.

    @interruptable()
    def _prepare_pipeline_fs(self):
        """Creates all of the directories necessary to run the pipeline, store
        temp files, and cache data"""
        server_logger.debug(self.p_log("PREPARING FILESYSTEM"))
        # Set the directories

        # References to paths on the nfs server
        self.state.ctx.pipeline.nfs_root_dir = f"/{self.state.ctx.idempotency_key}/"
        self.state.ctx.pipeline.nfs_cache_dir = f"{self.state.ctx.pipeline.nfs_root_dir}cache/"
        self.state.ctx.pipeline.nfs_docker_cache_dir = f"{self.state.ctx.pipeline.nfs_cache_dir}docker"
        self.state.ctx.pipeline.nfs_singularity_cache_dir = f"{self.state.ctx.pipeline.nfs_cache_dir}singularity"
        self.state.ctx.pipeline.nfs_git_cache_dir = f"{self.state.ctx.pipeline.nfs_cache_dir}git"
        self.state.ctx.pipeline.nfs_work_dir = f"{self.state.ctx.pipeline.nfs_root_dir}runs/{self.state.ctx.pipeline_run.uuid}/"

        # The pipeline root dir. All files and directories produced by a workflow
        # execution will reside here
        self.state.ctx.pipeline.root_dir = f"{BASE_WORK_DIR}{self.state.ctx.idempotency_key}/"
        os.makedirs(f"{self.state.ctx.pipeline.root_dir}", exist_ok=True)

        # Create the directories in which data between pipeline runs will be stored. This
        # will allow data to be reused or cached between runs
        self.state.ctx.pipeline.cache_dir = f"{self.state.ctx.pipeline.root_dir}cache/"
        os.makedirs(f"{self.state.ctx.pipeline.cache_dir}", exist_ok=True)

        # Create the docker and singularity cache dirs
        self.state.ctx.pipeline.docker_cache_dir = f"{self.state.ctx.pipeline.cache_dir}docker"
        os.makedirs(f"{self.state.ctx.pipeline.docker_cache_dir}", exist_ok=True)

        self.state.ctx.pipeline.singularity_cache_dir = f"{self.state.ctx.pipeline.cache_dir}singularity"
        os.makedirs(f"{self.state.ctx.pipeline.singularity_cache_dir}", exist_ok=True)

        # Create the github cache dir
        self.state.ctx.pipeline.git_cache_dir = f"{self.state.ctx.pipeline.cache_dir}git"
        os.makedirs(f"{self.state.ctx.pipeline.git_cache_dir}", exist_ok=True)

        # The directory for this particular run of the workflow
        self.state.ctx.pipeline.work_dir = f"{self.state.ctx.pipeline.root_dir}runs/{self.state.ctx.pipeline_run.uuid}/"
        os.makedirs(self.state.ctx.pipeline.work_dir, exist_ok=True)
        
        # Create a directory for the pipeline arguments
        self.state.ctx.pipeline.args_dir = f"{self.state.ctx.pipeline.work_dir}.args/"
        os.makedirs(self.state.ctx.pipeline.args_dir, exist_ok=True)

        # Create a directory for the environment variables
        self.state.ctx.pipeline.env_dir = f"{self.state.ctx.pipeline.work_dir}.env/"
        os.makedirs(self.state.ctx.pipeline.env_dir, exist_ok=True)

        # The log file for this pipeline run
        self.state.ctx.pipeline.log_file = f"{self.state.ctx.pipeline.work_dir}.logs.txt"
        
        # Set the work_dir on the WorkflowExecutor as well.
        # NOTE Will be used for cleaning up all the temporary files/dirs after
        # the state is reset. (Which means that ther will be no self.state.ctx.pipeline.work_dir)
        self.work_dir = self.state.ctx.pipeline.work_dir

        # Persist each arg to files in the pipeline file system
        arg_value_file_repo = self.container.load("ArgValueFileRepository")
        arg_repo = self.container.load("ArgRepository")
        for key in self.state.ctx.args:
            arg_value_file_repo.save(
                self.state.ctx.pipeline.args_dir + key,
                arg_repo.get_value_by_key(key)
            )

        # Persist each arg to files in the pipeline file system
        env_var_value_file_repo = self.container.load("EnvVarValueFileRepository")
        env_repo = self.container.load("EnvRepository")
        for key in self.state.ctx.env:
            env_var_value_file_repo.save(
                self.state.ctx.pipeline.env_dir + key,
                env_repo.get_value_by_key(key)
            )

    @interruptable()
    def _fetch_ready_tasks(self):
        ready_tasks = []
        threads = []
        for task in self.state.queue:
            if self._task_is_ready(task):
                ready_tasks.append(task)

        for task in ready_tasks:
            self._remove_from_queue(task)
            t = Thread(
                target=self._start_task,
                args=(task,)
            )
            threads.append(t)

        return threads

    @interruptable()
    def _task_is_ready(self, task):
        # All tasks without dependencies are ready immediately
        if len(task.depends_on) == 0: return True

        for dep in task.depends_on:
            dep_task = self._get_task_by_id(dep.id)
            if dep_task == None:
                raise Exception(f"Illegal dependency: Task with id '{dep.id}' doesn't exist")
            
            if dep_task.can_fail and dep_task.id in self.state.failed:
                continue

            if dep_task.id not in self.state.finished:
                return False

        return True
    
    @interruptable()
    def _get_task_by_id(self, task_id):
        return next(filter(lambda t: t.id == task_id,self.state.tasks), None)

    @interruptable()
    def _remove_from_queue(self, task):
        len(self.state.queue) == 0 or self.state.queue.pop(self.state.queue.index(task))

    @interruptable()
    def _register_executor(self, run_uuid, task, executor):
        self.state.executors[f"{run_uuid}.{task.id}"] = executor

    @interruptable()
    def _get_executor(self, run_uuid, task, default=None):
        return self.state.executors.get(f"{run_uuid}.{task.id}", None)

    @interruptable()
    def _deregister_executor(self, run_uuid, task):
        # Clean up the resources created by the task executor
        executor = self._get_executor(run_uuid, task)
        executor.cleanup()
        del self.state.executors[f"{run_uuid}.{task.id}"]
        # TODO use server logger below
        # self.state.ctx.logger.debug(self.t_log(task, "EXECUTOR DEREGISTERED"))

    @interruptable()
    def _get_executor(self, run_uuid, task):
        return self.state.executors[f"{run_uuid}.{task.id}"]
    
    def _cleanup_run(self):
        # TODO remove comment below and pass above
        # os.system(f"rm -rf {self.work_dir}")
        pass
    
    def terminate(self):
        # NOTE SIDE EFFECT. Triggers the _on_terminate_hook in the
        # reactive state. Will prevent all gets and sets to self.state
        # thereafter
        self.state.terminating = True

    def reset(self, terminated=False):
        self.state.reset()
        self._set_initial_state()
        if terminated:
            self.state.terminated = True

    @interruptable()
    def _setup_loggers(self):
        # Create the logger. NOTE Directly instantiating a Logger object
        # is recommended against in the documentation, however it makes sense
        # in this using getLogger method will create a new Logger for each
        # pipeline run
        run_logger = logging.Logger(self.state.ctx.pipeline_run.uuid)

        handler = logging.FileHandler(f"{self.state.ctx.pipeline.log_file}")
        handler.setFormatter(logging.Formatter("%(message)s"))

        run_logger.setLevel(logging.DEBUG)
        run_logger.addHandler(handler)
        self.state.ctx.logger = CompositeLogger([server_logger, run_logger])
        
    @interruptable(rollback="_reset_event_exchange")
    def _initialize_notification_handlers(self):
        self.state.ctx.logger.debug(self.p_log("INITIALIZING NOTIFICATION HANDLERS"))
        # Initialize the notification event handlers from plugins. Notification event handlers are used to persist updates about the
        # pipeline and its tasks
        for plugin in self._plugins:
            for middleware in plugin.notification_middlewares:
                try:
                    handler = middleware.handler(self.state.ctx)
                except Exception as e:
                    server_logger.exception(e.__cause__)
                    self.state.ctx.logger.error(f"Could not intialize notification middleware. Updates about the pipeline and its task will not be persisited. Error: {str(e)}")
                    return

                # Add the notification_handler as a subscriber to all events. When these events are "published"
                # by the workflow executor, the notification_handler will be passed the message to handle it.
                self.add_subscribers(
                    handler,
                    middleware.subscriptions
                )

    @interruptable(rollback="_reset_event_exchange")
    def _initialize_archivers(self):
        # No archivers specified. Return
        if len(self.state.ctx.archives) < 1: return

        self.state.ctx.logger.debug(self.p_log("INITIALIZING ARCHIVERS"))

        # TODO Handle for multiple archives
        self.state.ctx.archive = self.state.ctx.archives[0]

        archive_middlewares = [
            ArchiveMiddleware(
                "s3",
                handler=S3Archiver,
                subsciptions=[PIPELINE_COMPLETED, PIPELINE_FAILED, PIPELINE_TERMINATED]
            ),
            ArchiveMiddleware(
                "irods",
                handler=IRODSArchiver,
                subsciptions=[PIPELINE_COMPLETED, PIPELINE_FAILED, PIPELINE_TERMINATED]
            )
        ]
        
        # Add archive middleware from plugins to
        for plugin in self._plugins:
            archive_middlewares = [*archive_middlewares, *plugin.archive_middlewares]

        # Get and initialize the archiver
        middleware = next(filter(
            lambda middleware: middleware.type == self.state.ctx.archive.type,
            archive_middlewares
        ), None)

        if middleware == None:
            self.state.ctx.logger.error(self.p_log(f"FAILED TO INITIALIZE ARCHIVER: No Archive Middleware found with type {self.state.ctx.archive.type}")) 
            return
        
        try:
            handler = middleware.handler()
        except Exception as e:
            server_logger.exception(e.__cause__)
            self.state.ctx.logger.error(f"Could not intialize archive middleware. Pipeline results will not be persisted. Error: {str(e)}")
            return
        
        # Add the archiver to the subscribers list
        self.add_subscribers(
            handler,
            middleware.subscriptions
        )

    def _set_initial_state(self):
        # Non-reactive state
        self.work_dir = None
        self.can_start = False

    @interruptable()
    def _set_context(self, ctx):
        self.state.ctx = ctx

    def _reset_event_exchange(self):
        self.exchange.reset()

    # Hooks
    @method_hook
    def _on_change_ready_task(self, state):
        for t in state.ready_tasks:
            t.start()
            state.threads.append(t)

        # Remove the ready tasks
        state.ready_tasks = []


    @method_hook
    def _on_change_state(self, state):
        """Cleans up the resources and state of the WorkflowExecutor when terminated.

        This is invoked by the WorkflowExecutors ReactiveState object when the intercept 
        condition is met, i.e. when the 'terminate' method changes 'self.state.terminated' 
        to True.
        """
        if not state.terminating or state.terminated:
            return
        
        # Log the terminating status
        self.state.ctx.logger.info(self.p_log("TERMINATING"))

        # Publish the termination event
        self.publish(Event(PIPELINE_TERMINATED, self.state.ctx))

        # Set the skip flag for all tasks to True
        for task in self.state.ctx.pipeline.tasks:
            task.skip = True
        
        # Terminate the Task Executors
        for _, executor in state.executors.items():
            executor.terminate()
    
        self._cleanup_run()